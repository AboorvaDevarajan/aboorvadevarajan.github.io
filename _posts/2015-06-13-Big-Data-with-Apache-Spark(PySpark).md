---
layout: post
title: Big Data with Apache Spark (Week 2) Part I
---



<br/>
 **Introduction to Big Data with Apache Spark (PySpark)**

**Part 2:**

>**The Big Data Problem**

>**Hardware for Big Data**

>**Distribution of work in Big Data**

>**Handling Failures and Slow Machines**

>**Map Reduce and Complex Jobs**

>**Apache Spark**

Analysis Tools mainly includes :  UNIX Shell Commands, Pandas (Python), R

**Distribution of work in Big Data**

Huge quantities of log data generated by all sorts of devices opens immense potential for insight, but machine learning is needed to make sense of it.

The main problem in big data is that the huge data cannot be stored and processed in a single standalone system, and thus the distributed / clustered environment can be used.

**Example:  Counting the words locally**

One naive local approach is to use the hash table

    Initialize an empty hash {}
    for each word in sentence:
        if(there is no word key in hash):
           add a new entry as { key: word, value: 0 }
    	else:
    	   update the value of the entry { key: word , value: value+1}

The other way to do it in distributed environment is to use Map-Reduce:

The figure below shows the typical MapReduce processing flow. There are effectively three processes that go on here:
<center>
![enter image description here](http://www.rabidgremlin.com/data20/MapReduceWordCountOverview1.png =100x100)
</center>

***Map process*** – take some of data and split them between distinct keys and values, it performs the filtering task.

***Intermediate process*** – shuffle the keys into some logically order (this usually means sorting them)

***Reduce process*** – group data by keys and apply some function to their values,  Reduce() procedure that performs a summary operation (such as counting the number of students in each queue, yielding name frequencies)

**Handling Failures and Slow Machines**

MapReduce implementation handles the following problems:

> - Recovering from machine failures 
- Shuffling data between the Map and Reduce functions 
- Running the Map and Reduce functions on many machines 
- Recovering from slow machines


Map Reduce deals with failures and slow tasks by re-launching the tasks on other machines. This functionality is enabled by the requirement that individual tasks in a Map Reduce job are idempotent and have no side effects. These two properties mean that given the same input, re-executing a task will always produce the same result and will not change other state. So, the results and end condition of the system are the same, whether a task is executed once or a thousand times.


**Map Reduce and Disk I/O**

With distributed execution in Map Reduce,each stage that we perform passes through the hard drives. So, the initial step of the map reads data from the hard drive,processes it, and then writes it out to disk before we perform the shuffle operation to send data to the reducers.

At the reducers, they read the data in from disk,process it, and write the results out to disk.

As a result, if we have an iterative job, we're just repeating that, so we do stage one, stage two, stage three, then repeat stage one again-- then there'll be a lot of disk I/O operations for each repetition.

Each of the mappers is reading in data from disk, then writing data to disk, only to be read back again from disk by the reducers, written to disk by that reducer, then read again by the next stage's mapper, and so on.

The problem here's that disk I/O is very slow. So if we're running iterative jobs, they're primarily going to run at the speed of the disks instead of the speed of our CPUs.

This is a motivation for **Apache Spark**.

Because it's not just iterative jobs that we want to perform when we're doing data science, but also complex jobs like interactive mining or stream processing or interactive queries.

In each one of these cases, we start with some source data, and we repeatedly read that data and perform calculations and write data back out to disk. That high amount of disk I/O means things are going to run very slowly. Because, again, disk I/O is very slow.


Apache Spark is a new distributed execution engine that makes use of the in-memory storage rather than frequent I/O to the disks.

**The Spark Computing Framework**

It provides programming abstraction and parallel runtime to hide complexities of fault-tolerance and slow machines.

> - Spark will schedule the processes in the distributed environment.
 > - It also provides automatic recovery from the node failures and slow nodes.

<center>
![enter image description here](https://spark.apache.org/images/spark-stack.png =100x100)
</center>

"Using memory instead of disks offers two huge benefits. The first benefit is that memory is much faster than disks. The time to read or write a value to memory is only a few nanoseconds, while the time to read or write is several milliseconds - that means memory is a million times faster than disks. The second benefit is that keeping intermediate results in memory means that they do not have to be converted into a format that can be stored on disks. The process of converting a memory object to a disk object is called serialization and the process of converting a disk object to a memory object is called deserialization. Serializing and deserializing objects is a very expensive and time consuming process. Keeping intermediate results in memory avoids this significant overhead.

<center>
![enter image description here](http://blog.cloudera.com/wp-content/uploads/2013/11/spark1.png =100x100)
</center>

Taken together, the faster access times and avoidance of serialization/deserialization overhead make Spark much faster than Map Reduce - up to 100 times faster!"

**Hadoop Map-Reduce and Apache Spark Difference**
<center>
![enter image description here](http://s10.postimg.org/evyqtji2h/Screenshot_from_2015_06_13_16_22_02.png =100x100)
</center>
